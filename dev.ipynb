{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NewType\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn                 \n",
    "import torch.nn.functional as F           \n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "lr = 0.1\n",
    "batch_size = 10\n",
    "epochs = 100\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal_length  sepal_width  petal_length  petal_width    species\n",
      "0             5.1          3.5           1.4          0.2     setosa\n",
      "1             4.9          3.0           1.4          0.2     setosa\n",
      "2             4.7          3.2           1.3          0.2     setosa\n",
      "3             4.6          3.1           1.5          0.2     setosa\n",
      "4             5.0          3.6           1.4          0.2     setosa\n",
      "..            ...          ...           ...          ...        ...\n",
      "145           6.7          3.0           5.2          2.3  virginica\n",
      "146           6.3          2.5           5.0          1.9  virginica\n",
      "147           6.5          3.0           5.2          2.0  virginica\n",
      "148           6.2          3.4           5.4          2.3  virginica\n",
      "149           5.9          3.0           5.1          1.8  virginica\n",
      "\n",
      "[150 rows x 5 columns]\n",
      "(150,)\n",
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "\n",
    "df = pd.read_csv('iris-dataset.csv')\n",
    "print(df)\n",
    "\n",
    "test = df.to_numpy()\n",
    "\n",
    "y = test[:,4]\n",
    "print(y.shape)\n",
    "\n",
    "X = test[:,:4]\n",
    "print(X.shape) ### Problem is that I need to create test and training sets for both X and y...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "target\n",
      "frame\n",
      "target_names\n",
      "DESCR\n",
      "feature_names\n",
      "filename\n",
      "data_module\n"
     ]
    }
   ],
   "source": [
    "#Loading the data using sk-learn\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris=load_iris()\n",
    "\n",
    "for keys in iris.keys() :\n",
    "    print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "target names : ['setosa' 'versicolor' 'virginica']\n",
      "\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X=iris.data\n",
    "y=iris.target\n",
    "print(y)\n",
    "\n",
    "y = iris['target']\n",
    "print(y)\n",
    "\n",
    "print(f'target names : {iris.target_names}\\n')\n",
    "\n",
    "print(type(X))\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape : (120, 4)\n",
      "X_test shape : (30, 4)\n",
      "y_train shape : (120,)\n",
      "y_test shape : (30,)\n",
      "[1 2 1 0 2 2 1 0 0 1 1 1 2 0 1 0 2 1 1 2 1 2 2 0 2 2 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Splitting the training and the testing data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "print(f'X_train shape : {X_train.shape}')\n",
    "print(f'X_test shape : {X_test.shape}')\n",
    "print(f'y_train shape : {y_train.shape}')\n",
    "print(f'y_test shape : {y_test.shape}')\n",
    "\n",
    "print(y_test) # It does contain all the three types to my surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making it compatible with PyTorch\n",
    "\n",
    "X_train, y_train, X_test, y_test = map(\n",
    "    torch.tensor, (X_train, y_train, X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "tensor(0) tensor(2)\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train),type(X_test))\n",
    "print(type(y_train),type(y_test))\n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TensorDataset and DataLoader\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GET THE DATA:\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(test_size=0.2):\n",
    "    iris=load_iris()\n",
    "    X=iris.data\n",
    "    y=iris.target\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=test_size)\n",
    "    X_train, y_train, X_test, y_test = map(\n",
    "    torch.tensor, (X_train, y_train, X_test, y_test)\n",
    ")\n",
    "    return X_train,X_test,y_train,y_test\n",
    "\n",
    "def get_data(train_ds : TensorDataset, test_ds : TensorDataset, batch_size : int):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size, shuffle=True),\n",
    "        DataLoader(test_ds, batch_size=batch_size * 2),\n",
    "    )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network using nn.Module\n",
    "\n",
    "\n",
    "import torch.nn as nn    \n",
    "\n",
    "class IrisClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(IrisClassifier, self).__init__()\n",
    "        self.layer = nn.Linear(4, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.layer(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IrisClassifier(\n",
      "  (layer): Linear(in_features=4, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "my_model = IrisClassifier()\n",
    "\n",
    "print(my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(my_model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the model\n",
    "\n",
    "def get_model_with_optimizer(lr, momentum):\n",
    "    model = IrisClassifier()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Coding the training loop i.e. the fit() function\n",
    "\n",
    "def fit(model,opt,epochs,loss_function, train_dl, test_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for x_training_tensor, y_label in train_dl:\n",
    "            prediction = model(x_training_tensor.float())\n",
    "            loss = loss_function(prediction, y_label)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = sum(loss_function(model(x_training_tensor.float()), y_label) for x_training_tensor, y_label in test_dl)\n",
    "\n",
    "        print(epoch, valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False,  True, False, False, False, False, False, False, False, False])\n",
      "tensor([1, 1, 2, 2, 1, 2, 2, 2, 2, 1])\n",
      "loss : 1.1680114269256592\n",
      "\n",
      "loss.item() : 1.1680114269256592\n",
      "\n",
      "tensor([False, False, False, False, False, False,  True, False, False, False])\n",
      "tensor([2, 2, 2, 2, 1, 2, 1, 2, 2, 2])\n",
      "loss : 1.149756669998169\n",
      "\n",
      "loss.item() : 1.149756669998169\n",
      "\n",
      "tensor([False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([1, 1, 1, 1, 1, 2, 2, 2, 1, 1])\n",
      "loss : 1.1618506908416748\n",
      "\n",
      "loss.item() : 1.1618506908416748\n",
      "\n",
      "tensor([False, False, False, False, False,  True, False, False, False, False])\n",
      "tensor([1, 2, 2, 2, 2, 1, 1, 1, 2, 1])\n",
      "loss : 1.173966646194458\n",
      "\n",
      "loss.item() : 1.173966646194458\n",
      "\n",
      "tensor([False, False,  True,  True, False, False, False, False, False, False])\n",
      "tensor([2, 1, 1, 1, 1, 1, 2, 2, 1, 1])\n",
      "loss : 1.1400690078735352\n",
      "\n",
      "loss.item() : 1.1400690078735352\n",
      "\n",
      "tensor([False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([2, 2, 2, 2, 2, 2, 2, 1, 1, 2])\n",
      "loss : 1.1536670923233032\n",
      "\n",
      "loss.item() : 1.1536670923233032\n",
      "\n",
      "tensor([False,  True, False,  True, False, False, False, False, False, False])\n",
      "tensor([2, 1, 1, 1, 2, 0, 2, 2, 2, 2])\n",
      "loss : 1.1573952436447144\n",
      "\n",
      "loss.item() : 1.1573952436447144\n",
      "\n",
      "tensor([False, False, False, False, False, False,  True, False, False, False])\n",
      "tensor([2, 1, 2, 2, 2, 1, 1, 1, 1, 1])\n",
      "loss : 1.1626523733139038\n",
      "\n",
      "loss.item() : 1.1626523733139038\n",
      "\n",
      "tensor([False,  True, False, False, False, False, False,  True,  True,  True])\n",
      "tensor([2, 1, 2, 2, 2, 2, 2, 1, 1, 1])\n",
      "loss : 1.116477370262146\n",
      "\n",
      "loss.item() : 1.116477370262146\n",
      "\n",
      "tensor([False, False, False, False, False, False,  True,  True, False, False])\n",
      "tensor([2, 0, 1, 2, 2, 2, 1, 1, 2, 2])\n",
      "loss : 1.1599630117416382\n",
      "\n",
      "loss.item() : 1.1599630117416382\n",
      "\n",
      "tensor([False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([1, 1, 2, 1, 2, 1, 2, 1, 2, 1])\n",
      "loss : 1.1763795614242554\n",
      "\n",
      "loss.item() : 1.1763795614242554\n",
      "\n",
      "tensor([False, False, False, False, False, False, False, False, False, False])\n",
      "tensor([2, 1, 2, 2, 2, 1, 2, 2, 2, 2])\n",
      "loss : 1.1583998203277588\n",
      "\n",
      "loss.item() : 1.1583998203277588\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xd/10nyn_j13qs58rww11sjkgy80000gn/T/ipykernel_1079/1432828102.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(self.layer(x))\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "X_train,X_test,y_train,y_test = load_data()\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "test_ds = TensorDataset(X_test, y_test)\n",
    "train_dl, test_dl = get_data(train_ds, test_ds, batch_size)\n",
    "\n",
    "\n",
    "\n",
    "model, opt = get_model_with_optimizer(lr, momentum)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "for x,y in train_dl:\n",
    "    #print(f'x : {x}\\n y : {y}\\n')\n",
    "    output = model(x.float())\n",
    "    #print(f'x shape : {x.shape}\\n')\n",
    "    #print(f'y shape : {y.shape}\\n')\n",
    "    #print(f'output shape : {output.shape}\\n')\n",
    "    #print(f'output max min : {output.max()} , {output.min()}\\n')\n",
    "    #print(f'y max min : {y.max()} , {y.min()}\\n')\n",
    "    #prediction = output.max(1, keepdim=False)[1]\n",
    "    prediction = torch.max(output, 1)[1]\n",
    "    #print(f'prediction shape : {prediction.shape}\\n')\n",
    "    print(prediction == y)\n",
    "    print(prediction)\n",
    "    #print(f'sum(prediction == y) : {sum(prediction == y)}\\n')\n",
    "    loss = loss_function(output, y)\n",
    "    print(f'loss : {loss}\\n')\n",
    "    print(f'loss.item() : {loss.item()}\\n')\n",
    "    \n",
    "\n",
    "\n",
    "#fit(model,opt,epochs,loss_function, train_dl, test_dl)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('teaching')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2142a618935ee31125fbb97f06401dd4f60968dd5a549a21d575788564038767"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
